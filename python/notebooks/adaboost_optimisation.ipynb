{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall recap:\n",
    "\n",
    "Precision is the proprotion of all the times a model makes a correct positive prediction out of all the times that the model makes a positive prediction.\n",
    "\n",
    "$$\n",
    "precision=\\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "Recall is the proportion of times that the model detected a true positive out of all the times the model was correct:\n",
    "\n",
    "$$\n",
    "recall=\\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "In this case, it doesn't matter much if the model makes a lot of false negatives, as the next step in the process is that the negatives will be seen by human classifiers, obviously it would be good to minimise this however as human classifier time is at a premium. On the other hand, it matters very much that the model does not produce a lot of false positives because these will never get seen by human classifiers, and will be 'lost'.\n",
    "\n",
    "In our case a true positive would be identifying an 'ok' class\n",
    "A false positive would be incorrectly saying it is 'ok' when it is 'not-ok'\n",
    "\n",
    "A true negative would be correctly identifying a 'not-ok'\n",
    "A false negative would be incorrectly saying it is 'not-ok' when it is 'ok'\n",
    "\n",
    "A model optimised for precision would one that minimises false positives, which is what we are interested in in this case. A model optimised for precision would be one that minimises false negatives. We are less concerned with this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/transformed_data.pkl','rb') as f:\n",
    "    transformed_data = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "with open('../../data/targets.pkl','rb') as f:\n",
    "    targets = column_or_1d(pickle.load(f))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(np.c_[transformed_data, targets])\n",
    "\n",
    "#df.columns = ['start_date_unix', 'start_date_weekday', 'start_date_dayofyear', 'start_date_day', \n",
    "#                'start_date_week', 'start_date_month', 'start_date_hour','time_delta',\n",
    "#                'comment_why_you_came_strlength',\n",
    "#                'comment_why_you_came_capsratio', 'comment_where_for_help_strlength',\n",
    "#                'comment_where_for_help_capsratio','comment_further_comments_strlength',\n",
    "#                'comment_further_comments_capsratio','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=1337)\n",
    "for train_index, test_index in split.split(transformed_data, targets):\n",
    "    train_index = train_index\n",
    "    test_index=test_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_m =', len(test_index))\n",
    "print('test_m =', len(train_index))\n",
    "print('proportion of targets =',sum(targets[test_index])/len(targets[test_index]))\n",
    "print('proportion of targets =',sum(targets[train_index])/len(targets[train_index]))\n",
    "\n",
    "train_X = transformed_data[train_index]\n",
    "train_y = targets[train_index]\n",
    "test_X = transformed_data[test_index]\n",
    "test_y = targets[test_index]\n",
    "\n",
    "# Try to solve label shape error\n",
    "\n",
    "from sklearn.utils import column_or_1d\n",
    "train_y = column_or_1d(train_y)\n",
    "test_y = column_or_1d(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "abc = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.1, random_state=42)\n",
    "abc.fit(train_X, train_y)\n",
    "\n",
    "train_pred = abc.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = abc.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising the AdaBoost for high precision\n",
    "\n",
    "Since we are mostly interested in having a very precise model, and less worried about recall, here I tweak it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract a decision function for a single training example\n",
    "\n",
    "abc.decision_function(train_X[0,:].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "train_y_scores = cross_val_predict(abc, train_X, train_y, cv=3, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(train_y, train_y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.ylim([0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recalls, precisions, \"r-\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred_90 = (train_y_scores > 0.06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_pred_90"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Check the recall and precision with new targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print('precision:', round(precision_score(train_y, train_y_pred_90), 3))\n",
    "print('recall:', round(recall_score(train_y, train_y_pred_90), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_scores = cross_val_predict(abc, test_X, test_y, cv=3, method=\"decision_function\")\n",
    "test_y_pred_90 = (test_y_scores > 0.06)\n",
    "\n",
    "print('precision:', round(precision_score(test_y, test_y_pred_90), 3))\n",
    "print('recall:', round(recall_score(test_y, test_y_pred_90), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_y, test_y_pred_90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_y, test_y_pred_90))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the model often thinks that surveys are 'not ok' (and therefore will go on to human classifiers) when in fact they are 'ok' (this is fine because a human will later classify them), but the model very rarely thinks a survey is 'ok' when it is not ok. This is the ideal balance because it will prevent us from accidentally removing surveys from the human pool prematurely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimising parameters with RandomizedSearch/GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which variables can we tune in the RandomizedSearch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=400),\n",
    "    # TO access the parameters of the underlying Decision Tree, use\n",
    "    # base_estimator__...\n",
    "        'base_estimator__max_leaf_nodes': randint(1,10),\n",
    "        'base_estimator__max_features': randint(1,5),\n",
    "        'base_estimator__min_samples_split': randint(2,20),\n",
    "        'base_estimator__min_samples_leaf': randint(1,20),\n",
    "    }\n",
    "\n",
    "rnd_search = RandomizedSearchCV(abc, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='f1', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(round(mean_score,3), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_estimator_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifypipe",
   "language": "python",
   "name": "classifypipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
