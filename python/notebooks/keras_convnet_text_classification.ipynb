{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "BASE_DIR = '/Users/matthewupson/Documents/classifypipe/src/python/notebooks/'\n",
    "GLOVE_DIR = BASE_DIR + 'glove.6B/'\n",
    "TEXT_DATA_DIR = BASE_DIR + '20_newsgroup/'\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2\n",
    "ENGINE = os.getenv('DATABASE_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\n",
    "        (\n",
    "            \"select raw.respondent_id, concat_ws(', ', comment_why_you_came,\"\n",
    "            \"comment_where_for_help, comment_further_comments) as comment_combined,\"\n",
    "            \"vote, code from raw left join (select respondent_id,\"\n",
    "            \"vote from priority where coders is not null) p on\"\n",
    "            \"(raw.respondent_id = p.respondent_id) \"\n",
    "            \"left join (select code_id, code from codes) c on\"\n",
    "            \"(p.vote = c.code_id)\"\n",
    "        ),\n",
    "        con=ENGINE\n",
    "        )\n",
    "\n",
    "df = df.drop_duplicates(subset='respondent_id')\n",
    "df = df.dropna(axis=0, subset=['vote'])\n",
    "training_index = pd.read_csv('../../data/2017-06-24_training_set_indexes.csv')\n",
    "df = df[df['respondent_id'].isin(training_index['respondent_id'])]\n",
    "\n",
    "# Shouldn't do anything..\n",
    "assert len(df.loc[df['comment_combined'].isnull(),'comment_combined']) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement one-versus-all classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ova = df['vote'].copy().as_matrix()\n",
    "ova = np.array([0 if i not in [12] else 1 for i in ova])\n",
    "\n",
    "print(ova)\n",
    "print(ova.sum())\n",
    "print(len(ova))\n",
    "\n",
    "pd.Series(ova).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df['comment_combined'].tolist()\n",
    "labels_index = {'0': 'other', '4': 'service-problem', '12': 'ok'}\n",
    "labels = ova\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the total number of words to be used in word embedding\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "# Updates internal vocabulary based on a list of texts.\n",
    "# Required before using `texts_to_sequences` or `texts_to_matrix`.\n",
    "\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Transforms each text into a sequence of integers\n",
    "# Will only use words which the tokenizer knows (set by tokenizer.fit_on_texts()).\n",
    "# Will only use th etop 'num_words' set by the tokenizer\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads each sentence to the same lengyh as the longest sentence\n",
    "# Zeros are inserted where words are absent. Note that padding \n",
    "# starts on the left.\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Note that this will truncate at MAX_SEQUENCE_LENGTH leading to\n",
    "# an array of size:\n",
    "\n",
    "print(data.shape)\n",
    "print('Data now contains ', round((data == 0).sum() / (data.shape[0] * data.shape[1]) * 100), '% zeros.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape, '(classes)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "y_train = labels[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:]\n",
    "y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "print(labels[:-nb_validation_samples,0].sum())\n",
    "print(len(labels[:-nb_validation_samples,0]))\n",
    "print(labels[-nb_validation_samples:,0].sum())\n",
    "print(len(labels[-nb_validation_samples:,0]))\n",
    "\n",
    "print(labels[:-nb_validation_samples,0].sum()/len(labels[:-nb_validation_samples,0]))\n",
    "print(labels[-nb_validation_samples:,0].sum()/len(labels[-nb_validation_samples:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the embedding here using 400000 words with 100 dimensions\n",
    "# This is a pre-trained embedding (see link in top chunk)\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# Output is a dict with two keys: word and coefs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix, by setting words not found in \n",
    "# embedding matrix to zero. Set the size of the embdedding\n",
    "# using the EMBEDDING_DIM var.\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "logname = 'tf_logs/govuk_ova_ok_' + str(datetime.now())\n",
    "\n",
    "tb = TensorBoard(\n",
    "    log_dir=logname, histogram_freq=0, batch_size=32, \n",
    "    write_graph=True, write_grads=False, write_images=False, \n",
    "    embeddings_freq=0, embeddings_layer_names=None, \n",
    "    embeddings_metadata=None)\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3, \n",
    "    verbose=1, mode='auto')\n",
    "\n",
    "# Create keras embedding layer from pre-trained embedding\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "# Set input size for embedding layer\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(10)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "\n",
    "\n",
    "# Adjust number of output nodes here for OVA should be 2!\n",
    "\n",
    "preds = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adamax',\n",
    "              metrics=['acc'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_data=(x_val, y_val),\n",
    "          epochs=20, batch_size=128, callbacks=[tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(x_val, y_val)\n",
    "\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predi(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_val[:,1], test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(logname + '.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
