{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/transformed_data.pkl','rb') as f:\n",
    "    transformed_data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "with open('../../data/targets.pkl','rb') as f:\n",
    "    targets = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "transformed_data = np.c_[transformed_data,transformed_data[:,[9,11,13]].sum(axis=1)]\n",
    "transformed_data = np.c_[transformed_data,transformed_data[:,[10,12,14]].mean(axis=1)]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "strlength_cat = np.ceil(transformed_data[:,15] / 0.1)\n",
    "strlength_cat[strlength_cat >= 2] = 2.0\n",
    "strlength_cat = scaler.fit_transform(strlength_cat.reshape(-1, 1))\n",
    "transformed_data = np.c_[transformed_data, strlength_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_cat = transformed_data[:,7]\n",
    "delta_cat = np.ceil(delta_cat/0.0001)\n",
    "delta_cat[delta_cat > 1] = 2\n",
    "delta_cat = scaler.fit_transform(delta_cat.reshape(-1, 1))\n",
    "\n",
    "transformed_data = np.c_[transformed_data, delta_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add bias node to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = np.c_[np.ones((transformed_data.shape[0], 1)), transformed_data]\n",
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a stratified test and train set, ensuring that there is a good proportion of targets and non-targets in the test/training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=1337)\n",
    "for train_index, test_index in split.split(transformed_data, strlength_cat):\n",
    "    train_index = train_index\n",
    "    test_index=test_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_m =', len(test_index))\n",
    "print('test_m =', len(train_index))\n",
    "print('proportion of targets =',sum(targets[test_index])/len(targets[test_index]))\n",
    "print('proportion of targets =',sum(targets[train_index])/len(targets[train_index]))\n",
    "\n",
    "train_X = transformed_data[train_index]\n",
    "train_y = targets[train_index]\n",
    "test_X = transformed_data[test_index]\n",
    "test_y = targets[test_index]\n",
    "\n",
    "from sklearn.utils import column_or_1d \n",
    "train_y = column_or_1d(train_y)\n",
    "test_y = column_or_1d(test_y)\n",
    "\n",
    "# What about string length categories\n",
    "\n",
    "print(pd.Series(train_X[:,17]).value_counts() / train_X.shape[0])\n",
    "print(pd.Series(test_X[:,17]).value_counts() / test_X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train_X.shape[0]\n",
    "n = train_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 150\n",
    "\n",
    "tnorm=TruncatedNormal(mean=0.0, stddev=0.05, seed=None)\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(\n",
    "    layers.Dense(n_nodes, activation='elu', \n",
    "                 input_shape=(n,), kernel_initializer=tnorm))\n",
    "network.add(layers.Dense(n_nodes, activation='elu', input_shape=(n,)))\n",
    "network.add(layers.Dense(n_nodes, activation='elu', input_shape=(n,)))\n",
    "network.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "am = Adam(\n",
    "    lr=0.001, beta_1=0.9, beta_2=0.999, \n",
    "    epsilon=1e-08, decay=0.0)\n",
    "\n",
    "\n",
    "network.compile(optimizer=am,\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_y = to_categorical(train_y)\n",
    "test_y = to_categorical(test_y)\n",
    "\n",
    "# Write logs to tensorboard (note you need to start a server with\n",
    "# tensorboard --logdir ./tf_logs/)\n",
    "\n",
    "tb = TensorBoard(\n",
    "    log_dir='./tf_logs', histogram_freq=0, batch_size=32, \n",
    "    write_graph=True, write_grads=False, write_images=False, \n",
    "    embeddings_freq=0, embeddings_layer_names=None, \n",
    "    embeddings_metadata=None)\n",
    "\n",
    "# Employ early stopping as a means of regularisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    min_delta=0.001, \n",
    "    patience=10, verbose=1, \n",
    "    mode='auto')\n",
    "\n",
    "# 200 epochs seems to minimise the test set error.\n",
    "\n",
    "network.fit(\n",
    "    train_X, train_y, epochs=200, \n",
    "    batch_size=128, callbacks=[tb],\n",
    "    validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_X, test_y)\n",
    "\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = network.predict_classes(test_X)\n",
    "#69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(test_y[:,1], test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifypipe",
   "language": "python",
   "name": "classifypipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
