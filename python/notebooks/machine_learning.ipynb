{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall recap:\n",
    "\n",
    "Precision is the proprotion of all the times a model makes a correct positive prediction out of all the times that the model makes a positive prediction.\n",
    "\n",
    "$$\n",
    "precision=\\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "Recall is the proportion of times that the model detected a true positive out of all the times the model was correct:\n",
    "\n",
    "$$\n",
    "recall=\\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "In this case, it doesn't matter much if the model makes a lot of false negatives, as the next step in the process is that the negatives will be seen by human classifiers, obviously it would be good to minimise this however as human classifier time is at a premium. On the other hand, it matters very much that the model does not produce a lot of false positives because these will never get seen by human classifiers, and will be 'lost'.\n",
    "\n",
    "In our case a true positive would be identifying an 'ok' class\n",
    "A false positive would be incorrectly saying it is 'ok' when it is 'not-ok'\n",
    "\n",
    "A true negative would be correctly identifying a 'not-ok'\n",
    "A false negative would be incorrectly saying it is 'not-ok' when it is 'ok'\n",
    "\n",
    "A model optimised for precision would one that minimises false positives, which is what we are interested in in this case. A model optimised for precision would be one that minimises false negatives. We are less concerned with this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/transformed_data.pkl','rb') as f:\n",
    "    transformed_data = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "with open('../../data/targets.pkl','rb') as f:\n",
    "    targets = column_or_1d(pickle.load(f))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(np.c_[transformed_data, targets])\n",
    "\n",
    "df.columns = ['start_date_unix', 'start_date_weekday', 'start_date_dayofyear', 'start_date_day', \n",
    "                'start_date_week', 'start_date_month', 'start_date_hour','time_delta',\n",
    "                'comment_why_you_came_strlength',\n",
    "                'comment_why_you_came_capsratio', 'comment_where_for_help_strlength',\n",
    "                'comment_where_for_help_capsratio','comment_further_comments_strlength',\n",
    "                'comment_further_comments_capsratio','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=1337)\n",
    "for train_index, test_index in split.split(transformed_data, targets):\n",
    "    train_index = train_index\n",
    "    test_index=test_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_m =', len(test_index))\n",
    "print('test_m =', len(train_index))\n",
    "print('proportion of targets =',sum(targets[test_index])/len(targets[test_index]))\n",
    "print('proportion of targets =',sum(targets[train_index])/len(targets[train_index]))\n",
    "\n",
    "train_X = transformed_data[train_index]\n",
    "train_y = targets[train_index]\n",
    "test_X = transformed_data[test_index]\n",
    "test_y = targets[test_index]\n",
    "\n",
    "# Try to solve label shape error\n",
    "\n",
    "from sklearn.utils import column_or_1d\n",
    "train_y = column_or_1d(train_y)\n",
    "test_y = column_or_1d(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = log_reg.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = log_reg.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does cross validation make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(log_reg, test_X, test_y, \n",
    "                         scoring=\"f1\", cv=20)\n",
    "\n",
    "print(\"Mean f1 score:\", round(scores.mean(),2))\n",
    "print(\"Std f1 score:\", round(scores.std(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"rbf\")\n",
    "svm.fit(train_X, train_y)\n",
    "\n",
    "train_pred = svm.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = svm.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "dtc.fit(train_X, train_y)\n",
    "\n",
    "train_pred = dtc.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = dtc.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree seems to perform much better on this dataset. We are beyond dice roll territory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about an enseble of all the models so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_reg), ('svm', svm), ('dtc', dtc)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(train_X, train_y)\n",
    "train_pred = voting_clf.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = voting_clf.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not perform very well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(train_X, train_y)\n",
    "train_pred = rf.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = rf.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([df.columns,rf.feature_importances_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bc = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)\n",
    "bc.fit(train_X, train_y)\n",
    "bc.oob_score_\n",
    "\n",
    "train_pred = bc.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = bc.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "abc = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.1, random_state=42)\n",
    "abc.fit(train_X, train_y)\n",
    "\n",
    "train_pred = abc.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = abc.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random parameter search on Random Forest\n",
    "\n",
    "This is the best performing model out of the box, maybe a random parameter search will improve upon it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8)\n",
    "    }\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(rf, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cvres.cv_results_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classifypipe",
   "language": "python",
   "name": "classifypipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
