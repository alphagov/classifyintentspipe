{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall recap:\n",
    "\n",
    "Precision is the proprotion of all the times a model makes a correct positive prediction out of all the times that the model makes a positive prediction.\n",
    "\n",
    "$$\n",
    "precision=\\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "Recall is the proportion of times that the model detected a true positive out of all the times the model was correct:\n",
    "\n",
    "$$\n",
    "recall=\\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "In this case, it doesn't matter much if the model makes a lot of false negatives, as the next step in the process is that the negatives will be seen by human classifiers, obviously it would be good to minimise this however as human classifier time is at a premium. On the other hand, it matters very much that the model does not produce a lot of false positives because these will never get seen by human classifiers, and will be 'lost'.\n",
    "\n",
    "In our case a true positive would be identifying an 'ok' class\n",
    "A false positive would be incorrectly saying it is 'ok' when it is 'not-ok'\n",
    "\n",
    "A true negative would be correctly identifying a 'not-ok'\n",
    "A false negative would be incorrectly saying it is 'not-ok' when it is 'ok'\n",
    "\n",
    "A model optimised for precision would one that minimises false positives, which is what we are interested in in this case. A model optimised for precision would be one that minimises false negatives. We are less concerned with this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/transformed_data.pkl','rb') as f:\n",
    "    transformed_data = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "with open('../../data/targets.pkl','rb') as f:\n",
    "    targets = column_or_1d(pickle.load(f))\n",
    "    f.close()\n",
    "\n",
    "\n",
    "#df = pd.DataFrame(np.c_[transformed_data, targets])\n",
    "\n",
    "#df.columns = ['start_date_unix', 'start_date_weekday', 'start_date_dayofyear', 'start_date_day', \n",
    "#                'start_date_week', 'start_date_month', 'start_date_hour','time_delta',\n",
    "#                'comment_why_you_came_strlength',\n",
    "#                'comment_why_you_came_capsratio', 'comment_where_for_help_strlength',\n",
    "#                'comment_where_for_help_capsratio','comment_further_comments_strlength',\n",
    "#                'comment_further_comments_capsratio','target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delta_cat = transformed_data[:,7]\n",
    "plt.subplot(121)\n",
    "plt.hist(delta_cat)\n",
    "\n",
    "delta_cat = np.ceil(delta_cat/0.0001)\n",
    "delta_cat[delta_cat > 1] = 2\n",
    "delta_cat = scaler.fit_transform(delta_cat.reshape(-1, 1))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(delta_cat)\n",
    "plt.show()\n",
    "\n",
    "transformed_data = np.c_[transformed_data, delta_cat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=1337)\n",
    "for train_index, test_index in split.split(transformed_data, targets):\n",
    "    train_index = train_index\n",
    "    test_index=test_index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_m =', len(test_index))\n",
    "print('test_m =', len(train_index))\n",
    "print('proportion of targets =',sum(targets[test_index])/len(targets[test_index]))\n",
    "print('proportion of targets =',sum(targets[train_index])/len(targets[train_index]))\n",
    "\n",
    "train_X = transformed_data[train_index]\n",
    "train_y = targets[train_index]\n",
    "test_X = transformed_data[test_index]\n",
    "test_y = targets[test_index]\n",
    "\n",
    "# Try to solve label shape error\n",
    "\n",
    "from sklearn.utils import column_or_1d\n",
    "train_y = column_or_1d(train_y)\n",
    "test_y = column_or_1d(test_y)\n",
    "\n",
    "# What about string length categories\n",
    "\n",
    "print(pd.Series(train_X[:,9]).value_counts() / train_X.shape[0])\n",
    "print(pd.Series(test_X[:,9]).value_counts() / test_X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = log_reg.predict(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = log_reg.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(test_y, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does cross validation make a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(log_reg, test_X, test_y, \n",
    "                         scoring=\"f1\", cv=20)\n",
    "\n",
    "print(\"Mean f1 score:\", round(scores.mean(),2))\n",
    "print(\"Std f1 score:\", round(scores.std(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"rbf\")\n",
    "svm.fit(train_X, train_y)\n",
    "\n",
    "train_pred = svm.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = svm.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "dtc.fit(train_X, train_y)\n",
    "\n",
    "train_pred = dtc.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = dtc.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree seems to perform much better on this dataset. We are beyond dice roll territory, but it badly overfits the training data leading to poor performance on the tests dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42,n_jobs=-1, oob_score=True, n_estimators=500)\n",
    "rf.fit(train_X, train_y)\n",
    "\n",
    "print(rf.oob_score)\n",
    "\n",
    "test_pred = rf.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame([df.columns,rf.feature_importances_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bc = BaggingClassifier(\n",
    "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)\n",
    "bc.fit(train_X, train_y)\n",
    "bc.oob_score_\n",
    "\n",
    "train_pred = bc.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = bc.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier(random_state=42)\n",
    "etc.fit(train_X, train_y)\n",
    "train_pred = etc.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = etc.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "abc = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm=\"SAMME.R\", learning_rate=0.1, random_state=42)\n",
    "abc.fit(train_X, train_y)\n",
    "\n",
    "train_pred = abc.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = abc.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(train_X, train_y)\n",
    "\n",
    "train_pred = xgb.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = xgb.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about an ensemble of all the models so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('log_reg', log_reg),('ada', abc), ('randomForest', rf), ('bagging', bc)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(train_X, train_y)\n",
    "train_pred = voting_clf.predict(train_X)\n",
    "\n",
    "class_report = classification_report(train_y, train_pred)\n",
    "print(class_report)\n",
    "\n",
    "test_pred = voting_clf.predict(test_X)\n",
    "class_report = classification_report(test_y, test_pred)\n",
    "print(class_report)\n",
    "\n",
    "print(confusion_matrix(test_y, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomforest_pred = rf.predict_proba(test_X)\n",
    "adaboost_pred = abc.predict_proba(test_X)\n",
    "bagging_pred = bc.predict_proba(test_X)\n",
    "logreg_pred = log_reg.predict_proba(test_X)\n",
    "#xgb_pred = xgb.predict_proba(test_X)\n",
    "\n",
    "with open('../../data/nn_out.pkl', 'rb') as f:\n",
    "    dnn_pred = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "blended = np.c_[\n",
    "    randomforest_pred, adaboost_pred, bagging_pred, \n",
    "    dnn_pred]\n",
    "\n",
    "blended_model = LogisticRegression(random_state=1337)\n",
    "blended_model.fit(blended, test_y)\n",
    "\n",
    "blended_pred = blended_model.predict(blended)\n",
    "class_report = classification_report(test_y, blended_pred)\n",
    "print(class_report)\n",
    "blended_model.get_params()\n",
    "\n",
    "# Double check this with CV\n",
    "\n",
    "scores = cross_val_score(blended_model, test_X, test_y,\n",
    "                        scoring='accuracy',cv=10)\n",
    "\n",
    "print(round(scores.mean(),3))\n",
    "print(round(scores.std(),3))\n",
    "#72"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
