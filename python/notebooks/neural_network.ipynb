{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pandas.plotting import scatter_matrix\n",
    "import tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/transformed_data.pkl','rb') as f:\n",
    "    transformed_data = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "with open('../../data/targets.pkl','rb') as f:\n",
    "    targets = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "df = pd.DataFrame(np.c_[transformed_data, targets])\n",
    "\n",
    "df.columns = ['start_date_unix', 'start_date_weekday', 'start_date_dayofyear', 'start_date_day', \n",
    "                'start_date_week', 'start_date_month', 'start_date_hour','time_delta',\n",
    "                'comment_why_you_came_strlength',\n",
    "                'comment_why_you_came_capsratio', 'comment_where_for_help_strlength',\n",
    "                'comment_where_for_help_capsratio','comment_further_comments_strlength',\n",
    "                'comment_further_comments_capsratio','target']\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add bias node to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = np.c_[np.ones((transformed_data.shape[0], 1)), transformed_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a stratified test and train set, ensuring that there is a good proportion of targets and non-targets in the test/training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=1337)\n",
    "for train_index, test_index in split.split(transformed_data, targets):\n",
    "    train_index = train_index\n",
    "    test_index=test_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_m =', len(test_index))\n",
    "print('test_m =', len(train_index))\n",
    "print('proportion of targets =',sum(targets[test_index])/len(targets[test_index]))\n",
    "print('proportion of targets =',sum(targets[train_index])/len(targets[train_index]))\n",
    "\n",
    "train_X = transformed_data[train_index]\n",
    "train_y = targets[train_index]\n",
    "test_X = transformed_data[test_index]\n",
    "test_y = targets[test_index]\n",
    "\n",
    "# Try to solve label shape error\n",
    "\n",
    "from sklearn.utils import column_or_1d\n",
    "train_y = column_or_1d(train_y)\n",
    "test_y = column_or_1d(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train_X.shape[0]\n",
    "n = train_X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 300\n",
    "n_outputs = 2\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('tf_logs', tf.get_default_graph())\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer(X, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(X.get_shape()[1])\n",
    "        stddev = 2 / np.sqrt(n_inputs)\n",
    "        init = tf.truncated_normal((n_inputs, n_neurons), stddev=stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name=\"bias\")\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the ELU activation function\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.elu,\n",
    "                              name=\"hidden1\")\n",
    "    \n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.elu,\n",
    "                              name=\"hidden2\")\n",
    "    \n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    \n",
    "    # Set a random seed that can be reproduced\n",
    "    \n",
    "    np.random.seed(epoch * n_batches + batch_index)  # not shown in the book\n",
    "    \n",
    "    # Select random indexes from m (training examples) \n",
    "    \n",
    "    indices = np.random.randint(m, size=batch_size)  # not shown\n",
    "    \n",
    "    #Â Create batches\n",
    "    \n",
    "    X_batch = train_X[indices] # not shown\n",
    "    #y_batch = train_y.reshape(-1, 1)[indices] # not shown\n",
    "    y_batch = column_or_1d(train_y[indices]) # not shown\n",
    "    \n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "n_batches = 50\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(m // batch_size):\n",
    "            X_batch, y_batch = fetch_batch(epoch, iteration, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            if iteration % 100 == 0:\n",
    "                summary_str = accuracy.eval(feed_dict={X: test_X, y: test_y})\n",
    "                step = epoch * n_batches + iteration\n",
    "                \n",
    "                #file_writer.add_summary(summary_str,step)\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: test_X, y: test_y})\n",
    "\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./saver/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
